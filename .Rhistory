pal2 = brewer.pal(8,"Dark2")
png("wordcloud_posv1.png", width=12,height=8, units='in', res=300)
wordcloud(posCorpus, scale=c(5,.2),min.freq=30, max.words=150, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
#Wordcloud for negative terms
negCorpus = Corpus(VectorSource(newdocs_negative$comments))
negCorpus = tm_map(negCorpus, tolower)
negCorpus = tm_map(negCorpus, removeWords, c('will','can','thanks','better','get','also',
'well','good','now', stopwords('english')))
negCorpus = tm_map(negCorpus, removePunctuation)
negCorpus = tm_map(negCorpus, removeNumbers)
negCorpus = tm_map(negCorpus, stripWhitespace)
negCorpus = tm_map(negCorpus, PlainTextDocument)
negCorpus = Corpus(VectorSource(negCorpus))
#wordcloud(negCorpus, max.words = 200, scale=c(3, .1), colors=brewer.pal(6, "Dark2"))
# wordcloud
pal2 = brewer.pal(8,"Dark2")
png("wordcloud_negv1.png", width=12,height=8, units='in', res=300)
wordcloud(negCorpus, scale=c(5,.2),min.freq=20, max.words=150, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
rm(list=ls())
setwd("G:/Analytics/Edwisor/Edwisor/Advanced Predictive Analytics/R Code/Sentiment Analysis/New folder")
#load libraries
library(stringr)
library(tm)
library(wordcloud)
library(slam)
library(sentiment)
#Load comments/text
post = read.csv("Post.csv", header = T)
#Load defined stop words
#stop_words = read.csv("stopwords.csv", header = T)
#names(stop_words) = "StopWords"
#Delete the leading spaces
post$Post = str_trim(post$Post)
#Select only text column
post = data.frame(post[1:2000,2])
names(post) = "comments"
post$comments = as.character(post$comments)
##Pre-processing
#convert comments into corpus
postCorpus = Corpus(VectorSource(post$comments))
writeLines(as.character(postCorpus[[2]]))
#postCorpus = tm_map(postCorpus, PlainTextDocument)
#case folding
postCorpus = tm_map(postCorpus, tolower)
#remove stop words
postCorpus = tm_map(postCorpus, removeWords, stopwords('english'))
#remove punctuation marks
postCorpus = tm_map(postCorpus, removePunctuation)
#remove num bers
postCorpus = tm_map(postCorpus, removeNumbers)
#remove unnecesary spaces
postCorpus = tm_map(postCorpus, stripWhitespace)
#convert into plain text
postCorpus = tm_map(postCorpus, PlainTextDocument)
#create corpus
postCorpus = Corpus(VectorSource(postCorpus))
##wordcloud
#Remove the defined stop words
postCorpus_WC = postCorpus
postCorpus_WC = tm_map(postCorpus, removeWords, c('i','its','it','us','use','used','using','will','yes','say','can','take','one',
stopwords('english')))
#postCorpus_WC = tm_map(postCorpus_WC, removeWords, stop_words$StopWords)
#Word cloud
#wordcloud(postCorpus_WC, max.words = 200, scale=c(3, .1), colors=brewer.pal(6, "Dark2"))
#Another method to build wordcloud
pal2 = brewer.pal(8,"Dark2")
png("wordcloud1.png", width = 12, height = 8, units = 'in', res = 300)
wordcloud(postCorpus_WC, scale = c(5,.2), min.freq = 30, max.words = 150, random.order = FALSE, rot.per = .15, colors = pal2)
dev.off()
#Build document term matrix
tdm = TermDocumentMatrix(postCorpus)
#tdm_min = TermDocumentMatrix(postCorpus, control=list(weighting=weightTfIdf, minWordLength=4, minDocFreq=10))
#calculate the terms frequency
words_freq = rollup(tdm, 2, na.rm=TRUE, FUN = sum)
words_freq = as.matrix(words_freq)
words_freq = data.frame(words_freq)
words_freq$words = row.names(words_freq)
row.names(words_freq) = NULL
words_freq = words_freq[,c(2,1)]
names(words_freq) = c("Words", "Frequency")
#wordcloud(words_freq$Words, words_freq$Frequency)
#Most frequent terms which appears in atleast 700 times
findFreqTerms(tdm, 100)
#sentiment Analysis
#Another method
library(RSentiment)
df = calculate_sentiment(post$comments)
#Another method
#Install sentiment library using binay source
#install.packages("D:/sentiment_0.2.tar.gz", repos = NULL, type="source")
library(sentiment)
#classifying the corpus as negative and positive and neutral
polarity = classify_polarity(post$comments, algorithm = "bayes", verbose = TRUE)
polarity = data.frame(polarity)
#Attached sentiments to the comments
newdocs = cbind(post, polarity)
#separate the comments based on polarity
newdocs_positive = newdocs[which(newdocs$BEST_FIT == "positive"),]
newdocs_negative = newdocs[which(newdocs$BEST_FIT == "negative"),]
newdocs_neutral  = newdocs[which(newdocs$BEST_FIT == "neutral"),]
##Build word cloud for each  polarity
#Positive terms wordcloud
#Pre-processing
posCorpus = Corpus(VectorSource(newdocs_positive$comments))
posCorpus = tm_map(posCorpus, tolower)
posCorpus = tm_map(posCorpus, removeWords, stopwords('english'))
posCorpus = tm_map(posCorpus, removePunctuation)
posCorpus = tm_map(posCorpus, removeNumbers)
posCorpus = tm_map(posCorpus, stripWhitespace)
posCorpus = tm_map(posCorpus, PlainTextDocument)
posCorpus = Corpus(VectorSource(posCorpus))
#Remove the defined stop words
posCorpus = tm_map(posCorpus, removeWords, c('i','its','it','us','use','used','using','will','can',
stopwords('english')))
#posCorpus = tm_map(posCorpus, removeWords, stop_words$StopWords)
#wordcloud(posCorpus, max.words = 200, scale=c(3, .1), colors=brewer.pal(6, "Dark2"))
# wordcloud
pal2 = brewer.pal(8,"Dark2")
png("wordcloud_posv1.png", width=12,height=8, units='in', res=300)
wordcloud(posCorpus, scale=c(5,.2),min.freq=30, max.words=150, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
#Wordcloud for negative terms
negCorpus = Corpus(VectorSource(newdocs_negative$comments))
negCorpus = tm_map(negCorpus, tolower)
negCorpus = tm_map(negCorpus, removeWords, c('will','can','thanks','better','get','also',
'well','good','now', stopwords('english')))
negCorpus = tm_map(negCorpus, removePunctuation)
negCorpus = tm_map(negCorpus, removeNumbers)
negCorpus = tm_map(negCorpus, stripWhitespace)
negCorpus = tm_map(negCorpus, PlainTextDocument)
negCorpus = Corpus(VectorSource(negCorpus))
#wordcloud(negCorpus, max.words = 200, scale=c(3, .1), colors=brewer.pal(6, "Dark2"))
# wordcloud
pal2 = brewer.pal(8,"Dark2")
png("wordcloud_negv1.png", width=12,height=8, units='in', res=300)
wordcloud(negCorpus, scale=c(5,.2),min.freq=20, max.words=150, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
setwd("G:/Analytics/Edwisor/Edwisor/Advanced Predictive Analytics/R")
rm(list=ls())
setwd("G:/Analytics/Edwisor/Edwisor/Advanced Predictive Analytics/R")
install.packages("devtools")
library(devtools)
install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
require (Rfacebook)
require ("Rfacebook")
fb_oauth <- fbOAuth(app_id="366446420429912", app_secret="bf6a026cb2608f409cf5c1a0f28540c7",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="366446420429912", app_secret="bf6a026cb2608f409cf5c1a0f28540c7",extended_permissions = TRUE)
rm(list = ls())
library(Rfacebook)
token = "EAACEdEose0cBAJEbU0tff3wBZB1bOCpUTl6PtB0C2bKuNqoeGbeSIayd8qxzcqRvEEHwy8xZCSEck6aAD7CuG9DyoKuIkZB0CMEujNdK8TvZB2TNUBp3KYGSTGgMuZAtGSPlfDnUZA5tEBPSFZBo190vmh9FRkC0YSZAbHhvIJ1CNb9yLtTlM4gZAXpQW08OP8WtgduKuGuVZBVQZDZD"
me = getUsers("me",token, private_info = T)
me$name
me$picture
me$hometown
me$gender
me$birthday
me$username
rm(list = ls())
library(Rfacebook)
token = "EAACEdEose0cBAJEbU0tff3wBZB1bOCpUTl6PtB0C2bKuNqoeGbeSIayd8qxzcqRvEEHwy8xZCSEck6aAD7CuG9DyoKuIkZB0CMEujNdK8TvZB2TNUBp3KYGSTGgMuZAtGSPlfDnUZA5tEBPSFZBo190vmh9FRkC0YSZAbHhvIJ1CNb9yLtTlM4gZAXpQW08OP8WtgduKuGuVZBVQZDZD"
me = getUsers("me",token, private_info = T)
me$name
me$hometown
me$gender
me$birthday
me$username
token = "EAACEdEose0cBAJEbU0tff3wBZB1bOCpUTl6PtB0C2bKuNqoeGbeSIayd8qxzcqRvEEHwy8xZCSEck6aAD7CuG9DyoKuIkZB0CMEujNdK8TvZB2TNUBp3KYGSTGgMuZAtGSPlfDnUZA5tEBPSFZBo190vmh9FRkC0YSZAbHhvIJ1CNb9yLtTlM4gZAXpQW08OP8WtgduKuGuVZBVQZDZD"
me = getUsers("me",token, private_info = T)
token = "EAACEdEose0cBAF8eZBR8DOTdyE90zKDyfZC7drZCZCCAkJoukb9K9j8ZB4s1OY1WT5aZAZBbv21Evfu60YwQcEmjrlGYs003rddN1LqykXfch3Klbvlm5MJJuRXXLuLy7JqDoGMZChh2cwcwP9na1aJ6sjwIKSZAKmg7MChbwoQEtvZCulra4C6uIIEGWApfcbGMAZD"
me = getUsers("me",token, private_info = T)
me$name
me$hometown
me$gender
me$birthday
me$username
me$relationship_status
#Getting data from public profiles
obama <- getPage("barackobama",token)
obama$story
obama$created_time
mahesh <- getPage("Mahesh Babu", token)
mahesh <- getPage("maheshbabu", token)
mahesh <- getPage("narendramodi", token)
modi <- getPage("narendramodi", token)
rm(list = mahesh)
rm(list = ls())
library(Rfacebook)
token = "EAACEdEose0cBAF8eZBR8DOTdyE90zKDyfZC7drZCZCCAkJoukb9K9j8ZB4s1OY1WT5aZAZBbv21Evfu60YwQcEmjrlGYs003rddN1LqykXfch3Klbvlm5MJJuRXXLuLy7JqDoGMZChh2cwcwP9na1aJ6sjwIKSZAKmg7MChbwoQEtvZCulra4C6uIIEGWApfcbGMAZD"
me = getUsers("me",token, private_info = T)
me$name
me$hometown
me$gender
me$birthday
me$username
me$relationship_status
#Getting data from public profiles
obama <- getPage("barackobama",token)
obama$story
obama$created_time
modi <- getPage("narendramodi", token)
View(modi)
write.csv(modi, file = 'modi.csv')
friends = getFriends(token)
View(friends)
View(friends)
KTR = getPage("KTR")
KTR = getPage("KTR",token)
KTR = getPage("KalvakuntlaTarakaRamaRao",token)
View(KTR)
install.packages('tableaur')
library(tableaur)
#Clear workspace
rm(list = ls())
#Set the working directory
setwd('C:/Users/chait/Desktop/Project2')
#Load Libraries
x = c('ggplot2', 'corrgram', 'DMwR', 'caret', 'randomForest', 'unbalanced', 'C50', 'dummies',
'e1071', 'Information','MASS','rpart', 'gbm', 'ROSE')
#install.packages(x)
lapply(x, require, character.only = TRUE)
rm(x)
#Read the dataset
df = read.csv('day.csv')
#Data preprocessing
sum(is.na(df))
head(df)
str(df)
summary(df)
colnames(df)
df[,1:9] =as.data.frame(lapply(df[,1:9],as.factor))
#Seperating the numeric data
numeric_index = sapply(df,is.numeric)
numeric_data = df[,numeric_index]
cnames = colnames(numeric_data)
cnames = cnames[1:6] #Removing the dependent variable
#Exploratory data analysis
for(i in 1:ncol(numeric_data))
{
hist(numeric_data[,i],main = paste("Histogram of" , cnames[i]),col='green',border = 'black',xlab = cnames[i])
}
#Scatterplot of numeric variables Vs Dependent variable
#Exploratory data analysis
for(i in 1:length(cnames))
{
x = df[,cnames[i]]
plot(x , y = df$cnt, main = paste("cnt vs ", cnames[i]),col = "blue")
}
#Smooth Scatter plot describing the relation ship between dependent and numeric variables
for(i in 1:(length(cnames)))
{
scatter.smooth(x=df[,cnames[i]],y=df$cnt,xlab = cnames[i],ylab = "cnt",col = "blue", main=paste("Smooth Scatter plot of cnt Vs ",cnames[i]))
}
#Outlier Analysis
#Boxplot of numeirc variables
for(i in 1:(length(cnames)))
{
boxplot(df[,cnames[i]],main = paste("Box Plot of ",cnames[i]),col = "green", outlier.color = "red", outcol ="red",sub = paste("Outliers: ",length(boxplot.stats(df[,cnames[i]])$out)))
}
for( i in cnames)
{
print(i)
val = df[,i][df[,i] %in% boxplot.stats(df[,i])$out]
df[,i][df[,i] %in% val] = NA
}
sum(is.na(df))
#Imputing the missing values with knnImputation
df = knnImputation(df, k=80)
#Feature Selection
library(corrplot)
library(corrgram)
M= cor(numeric_data[,-7])
corrplot(M,method = "circle")
corrplot(M, type="upper")
#VIF
library(usdm)
#vif(df[,-16])
vifcor(numeric_data[,-10],th = 0.9)
df1 = subset(df, select = -c(atemp,instant,dteday))
#Feature scaling
df[,i] = (df[,"casual"]- min(df[,"casual"]))/
(max(df[,"casual"] - min(df[,"casual"])))
df[,i] = (df[,"registered"]- min(df[,"registered"]))/
(max(df[,"registered"] - min(df[,"registered"])))
#Dividing Test and train datasets
train_index <- sample(1:nrow(df1), 0.8 * nrow(df1))
test_index <- setdiff(1:nrow(df1), train_index)
# Build X_train, y_train, X_test, y_test
X_train <- df1[train_index, -13]
y_train <- df1[train_index, "cnt"]
X_test <- df1[test_index, -13]
y_test <- df1[test_index, "cnt"]
df_test = data.frame(X_train,y_train)
colnames(df_test)[13] = "cnt"
#Linear Model
LinearModel = lm(cnt~., data = df_test)
summary(LinearModel)
confint(LinearModel)
Y_pred = predict(LinearModel,newdata = X_test)
predict(LinearModel,type = "terms")
X_test <- df1[test_index[,12:13], -13]
X_test <- df1[test_index[,13], -13]
#Clear workspace
rm(list = ls())
#Set the working directory
setwd('C:/Users/chait/Desktop/Project2')
#Load Libraries
x = c('ggplot2', 'corrgram', 'DMwR', 'caret', 'randomForest', 'unbalanced', 'C50', 'dummies',
'e1071', 'Information','MASS','rpart', 'gbm', 'ROSE')
#install.packages(x)
lapply(x, require, character.only = TRUE)
rm(x)
#Read the dataset
df = read.csv('day.csv')
#Data preprocessing
sum(is.na(df))
head(df)
str(df)
summary(df)
colnames(df)
df[,1:9] =as.data.frame(lapply(df[,1:9],as.factor))
#Seperating the numeric data
numeric_index = sapply(df,is.numeric)
numeric_data = df[,numeric_index]
cnames = colnames(numeric_data)
cnames = cnames[1:6] #Removing the dependent variable
#Exploratory data analysis
for(i in 1:ncol(numeric_data))
{
hist(numeric_data[,i],main = paste("Histogram of" , cnames[i]),col='green',border = 'black',xlab = cnames[i])
}
#Scatterplot of numeric variables Vs Dependent variable
#Exploratory data analysis
for(i in 1:length(cnames))
{
x = df[,cnames[i]]
plot(x , y = df$cnt, main = paste("cnt vs ", cnames[i]),col = "blue")
}
#Smooth Scatter plot describing the relation ship between dependent and numeric variables
for(i in 1:(length(cnames)))
{
scatter.smooth(x=df[,cnames[i]],y=df$cnt,xlab = cnames[i],ylab = "cnt",col = "blue", main=paste("Smooth Scatter plot of cnt Vs ",cnames[i]))
}
#Outlier Analysis
#Boxplot of numeirc variables
for(i in 1:(length(cnames)))
{
boxplot(df[,cnames[i]],main = paste("Box Plot of ",cnames[i]),col = "green", outlier.color = "red", outcol ="red",sub = paste("Outliers: ",length(boxplot.stats(df[,cnames[i]])$out)))
}
for( i in cnames)
{
print(i)
val = df[,i][df[,i] %in% boxplot.stats(df[,i])$out]
df[,i][df[,i] %in% val] = NA
}
sum(is.na(df))
#Imputing the missing values with knnImputation
df = knnImputation(df, k=80)
#Feature Selection
library(corrplot)
library(corrgram)
M= cor(numeric_data[,-7])
corrplot(M,method = "circle")
corrplot(M, type="upper")
#VIF
library(usdm)
#vif(df[,-16])
vifcor(numeric_data[,-10],th = 0.9)
df1 = subset(df, select = -c(atemp,instant,dteday))
#Feature scaling
df[,i] = (df[,"casual"]- min(df[,"casual"]))/
(max(df[,"casual"] - min(df[,"casual"])))
df[,i] = (df[,"registered"]- min(df[,"registered"]))/
(max(df[,"registered"] - min(df[,"registered"])))
#Dividing Test and train datasets
train_index <- sample(1:nrow(df1), 0.8 * nrow(df1))
test_index <- setdiff(1:nrow(df1), train_index)
# Build X_train, y_train, X_test, y_test
X_train <- df1[train_index, -13]
y_train <- df1[train_index, "cnt"]
X_test <- df1[test_index, -13]
y_test <- df1[test_index, "cnt"]
df_test = data.frame(X_train,y_train)
colnames(df_test)[13] = "cnt"
#Linear Model
LinearModel = lm(cnt~., data = df_test)
summary(LinearModel)
confint(LinearModel)
Y_pred = predict(LinearModel,newdata = X_test)
predict(LinearModel,type = "terms")
X_test = X_test.loc[12,]
X_test = X_test.iloc[12,]
X_test = X_test[12,]
X_test <- df1[test_index, -13]
X_test = X_test[12:14,]
View(X_test)
df_test = data.frame(X_test,y_test)
LinearModel = lm(cnt~., data = df_test)
df_train = data.frame(X_train,y_train)
LinearModel = lm(cnt~., data = df_train)
df_train = data.frame(X_train,y_train)
colnames(df_train)[13] = "cnt"
#Linear Model
LinearModel = lm(cnt~., data = df_train)
Y_pred = predict(LinearModel,newdata = X_test)
Y_pred
Y_test = Y_test[12:14,]
y_test <- df1[test_index, "cnt"]
Y_test = Y_test[12:14,]
y_test = y_test[12:14,]
y_test = y_test[12:14]
Y_pred
y_test
#Clear workspace
rm(list = ls())
#Set the working directory
setwd('C:/Users/chait/Desktop/Project2')
#Load Libraries
x = c('ggplot2', 'corrgram', 'DMwR', 'caret', 'randomForest', 'unbalanced', 'C50', 'dummies',
'e1071', 'Information','MASS','rpart', 'gbm', 'ROSE')
#install.packages(x)
lapply(x, require, character.only = TRUE)
rm(x)
#Read the dataset
df = read.csv('day.csv')
#Data preprocessing
sum(is.na(df))
head(df)
str(df)
summary(df)
colnames(df)
df[,1:9] =as.data.frame(lapply(df[,1:9],as.factor))
#Seperating the numeric data
numeric_index = sapply(df,is.numeric)
numeric_data = df[,numeric_index]
cnames = colnames(numeric_data)
cnames = cnames[1:6] #Removing the dependent variable
#Exploratory data analysis
for(i in 1:ncol(numeric_data))
{
hist(numeric_data[,i],main = paste("Histogram of" , cnames[i]),col='green',border = 'black',xlab = cnames[i])
}
#Scatterplot of numeric variables Vs Dependent variable
#Exploratory data analysis
for(i in 1:length(cnames))
{
x = df[,cnames[i]]
plot(x , y = df$cnt, main = paste("cnt vs ", cnames[i]),col = "blue")
}
#Smooth Scatter plot describing the relation ship between dependent and numeric variables
for(i in 1:(length(cnames)))
{
scatter.smooth(x=df[,cnames[i]],y=df$cnt,xlab = cnames[i],ylab = "cnt",col = "blue", main=paste("Smooth Scatter plot of cnt Vs ",cnames[i]))
}
#Outlier Analysis
#Boxplot of numeirc variables
for(i in 1:(length(cnames)))
{
boxplot(df[,cnames[i]],main = paste("Box Plot of ",cnames[i]),col = "green", outlier.color = "red", outcol ="red",sub = paste("Outliers: ",length(boxplot.stats(df[,cnames[i]])$out)))
}
for( i in cnames)
{
print(i)
val = df[,i][df[,i] %in% boxplot.stats(df[,i])$out]
df[,i][df[,i] %in% val] = NA
}
sum(is.na(df))
#Imputing the missing values with knnImputation
df = knnImputation(df, k=80)
#Feature Selection
library(corrplot)
library(corrgram)
M= cor(numeric_data[,-7])
corrplot(M,method = "circle")
corrplot(M, type="upper")
#VIF
library(usdm)
#vif(df[,-16])
vifcor(numeric_data[,-10],th = 0.9)
df1 = subset(df, select = -c(atemp,instant,dteday))
#Feature scaling
df[,i] = (df[,"casual"]- min(df[,"casual"]))/
(max(df[,"casual"] - min(df[,"casual"])))
df[,i] = (df[,"registered"]- min(df[,"registered"]))/
(max(df[,"registered"] - min(df[,"registered"])))
#Dividing Test and train datasets
train_index <- sample(1:nrow(df1), 0.8 * nrow(df1))
test_index <- setdiff(1:nrow(df1), train_index)
# Build X_train, y_train, X_test, y_test
X_train <- df1[train_index, -13]
y_train <- df1[train_index, "cnt"]
X_test <- df1[test_index, -13]
y_test <- df1[test_index, "cnt"]
df_train = data.frame(X_train,y_train)
colnames(df_train)[13] = "cnt"
#Linear Model
LinearModel = lm(cnt~., data = df_train)
summary(LinearModel)
confint(LinearModel)
Y_pred = predict(LinearModel,newdata = X_test)
predict(LinearModel,type = "terms")
X_test = X_test[12:14,]
y_test = y_test[12:14]
Y_pred = predict(LinearModel,newdata = X_test)
write.csv(X_test, 'SampleInputR.csv',index = FALSE)
write.csv(X_test, 'SampleInputR.csv')
y_test
predict(LinearModel,type = "terms")
predict(LinearModel,type = "terms")
Y_pred = predict(LinearModel,newdata = X_test)
Y_pred
